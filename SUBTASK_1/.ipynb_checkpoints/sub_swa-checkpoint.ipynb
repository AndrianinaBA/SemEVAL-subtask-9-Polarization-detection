{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d57b233-3ff1-4f61-b1cd-efb77e72c93f",
   "metadata": {},
   "source": [
    "## Subtask 1 - Polarization detection\n",
    "\n",
    "This is a binary classification to determine whether a post contains polarized content (Polarized or Not Polarized)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d23483-afb0-4661-a208-8375d9a93b46",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "792a50d7-5f74-4eeb-801f-c53d0b3601af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f3379a5-e320-4740-9a3c-c3a202f87588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "# Python\n",
    "random.seed(SEED)\n",
    "\n",
    "# Numpy\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch (CPU)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# PyTorch (GPU)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)  # if you have multiple GPUs\n",
    "\n",
    "# Ensure deterministic behavior\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e38fb187-d23e-4a8d-a13c-e92995dcf966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/l4u141df?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7c8d40449d20>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# Disable wandb logging for this script\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2be8548-cdd8-428b-8231-4451fa113fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>swa_53de6a7a4d0123b5755da79d8d97a82f</td>\n",
       "      <td>uwizi rt kenyan rao akishinda nitachinja kuku ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>swa_ee2533cb334df97236ea2bcfda0d6823</td>\n",
       "      <td>wakikuyu ndio wako na manyumba za kukodeshwa t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>swa_1dd81b5985840a55b1ab292aa65d11a8</td>\n",
       "      <td>wakikuyu ni wezi power hungry and this time we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>swa_18589adc3945e20c5e5c61e10245fad1</td>\n",
       "      <td>wakikuyu sijui shida yenu ni nini kuogopa rail...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>swa_aee76fc4cd1c6c6c09e19ba5ddd3901a</td>\n",
       "      <td>wakikuyu walisogwa hwakuumbwa</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  swa_53de6a7a4d0123b5755da79d8d97a82f   \n",
       "1  swa_ee2533cb334df97236ea2bcfda0d6823   \n",
       "2  swa_1dd81b5985840a55b1ab292aa65d11a8   \n",
       "3  swa_18589adc3945e20c5e5c61e10245fad1   \n",
       "4  swa_aee76fc4cd1c6c6c09e19ba5ddd3901a   \n",
       "\n",
       "                                                text  polarization  \n",
       "0  uwizi rt kenyan rao akishinda nitachinja kuku ...             1  \n",
       "1  wakikuyu ndio wako na manyumba za kukodeshwa t...             1  \n",
       "2  wakikuyu ni wezi power hungry and this time we...             1  \n",
       "3  wakikuyu sijui shida yenu ni nini kuogopa rail...             1  \n",
       "4                      wakikuyu walisogwa hwakuumbwa             1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the training and validation data for subtask 1\n",
    "\n",
    "train = pd.read_csv('train/swa.csv')\n",
    "train_plus = pd.read_csv('swa_backtranslated.csv')\n",
    "\n",
    "# dev = pd.read_csv('dev/swa.csv')\n",
    "# val = pd.read_csv('train/swa.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "797d1f58-c881-4fd2-a6ca-514f47e1762b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the dataset class by inheriting from torch.utils.data.Dataset\n",
    "class PolarizationDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self,texts,labels,tokenizer,max_length =128):\n",
    "    self.texts=texts\n",
    "    self.labels=labels\n",
    "    self.tokenizer= tokenizer\n",
    "    self.max_length = max_length # Store max_length\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    text = self.texts[idx]\n",
    "    label = self.labels[idx]\n",
    "\n",
    "    # Remove return_tensors='pt' - let DataCollator handle it\n",
    "    encoding = self.tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        max_length=self.max_length\n",
    "    )\n",
    "\n",
    "    encoding['label'] = label  # Use 'label' not 'labels'\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1131c8c2-2c3a-4975-8862-17f125065ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "MODEL_NAME = 'distilroberta-base' #  xlm-roberta-base\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66555a6e-a104-499d-a92f-d46bc2c2317d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_to_use = train.sample(n=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d1f24cd-7471-4d75-8477-94d2e51917ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data before adding the back-translation :  5000\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "train_polarization = []\n",
    "\n",
    "\n",
    "train_data.extend(train_to_use['text'].tolist())\n",
    "train_polarization.extend(train_to_use['polarization'].tolist())\n",
    "\n",
    "print(\"Total number of data before adding the back-translation : \", len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619842a0-31e4-4f6d-a1b1-fe2873fd5aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "back_tranlated_sampled = train_plus.sample(n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c292f82-ba31-49ed-8d32-3f8fe78b7a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "back_tranlated_sampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd43c9fa-000e-4de7-90a3-73b4f678f10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the data after adding new dataset : 5700\n"
     ]
    }
   ],
   "source": [
    "back_translated = back_tranlated_sampled['text'].tolist()\n",
    "back_translated_labels = back_tranlated_sampled['polarization'].tolist()\n",
    "\n",
    "add_to_train = back_translated[:-300]\n",
    "labels_train = back_translated_labels[:-300]\n",
    "\n",
    "val = back_translated[-300:]\n",
    "labels_val = back_translated_labels[-300:]\n",
    "labels_val_to_int = [int(elt) for elt in labels_val]\n",
    "\n",
    "# Adding the new dataset into the training set\n",
    "train_data.extend(add_to_train)\n",
    "train_polarization.extend(labels_train)\n",
    "\n",
    "print(\"Length of the data after adding new dataset :\", len(train_data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea983810-17e5-411a-bb3f-ee2567358ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "\n",
    "train_dataset = PolarizationDataset(train_data, train_polarization, tokenizer)\n",
    "val_dataset = PolarizationDataset(val, labels_val, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770cb38-0c83-4482-a097-876c3b477989",
   "metadata": {},
   "source": [
    "## Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbc597b1-0dff-497d-af4c-ed8db9e89472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True,\n",
    "    #local_files_only=True # Only if model is downloaded locally\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8dd6b3-7a35-429f-b61d-527e45b0f3bc",
   "metadata": {},
   "source": [
    "## Defining training argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a7b9485-fdde-4c08-a4ac-d707ea24d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics function\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    return {'f1_macro': f1_score(p.label_ids, preds, average='macro')}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "        output_dir=f\"./\",\n",
    "        num_train_epochs=4,\n",
    "        learning_rate=1e-5,\n",
    "        per_device_train_batch_size=32,\n",
    "        per_device_eval_batch_size=8,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_steps=100,\n",
    "        disable_tqdm=False\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03639b97-3e30-45e5-9a30-8032df9b2bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='716' max='716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [716/716 05:56, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646700</td>\n",
       "      <td>0.579089</td>\n",
       "      <td>0.709739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.524100</td>\n",
       "      <td>0.531535</td>\n",
       "      <td>0.726070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.456400</td>\n",
       "      <td>0.462634</td>\n",
       "      <td>0.769875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.431500</td>\n",
       "      <td>0.450419</td>\n",
       "      <td>0.779755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro F1 score on validation set: 0.7797552836484983\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset,            # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    "    data_collator=DataCollatorWithPadding(tokenizer) # Data collator for dynamic padding\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Macro F1 score on validation set: {eval_results['eval_f1_macro']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10d6c22d-2e20-4a84-bfc2-c6479fd42821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     id  polarization\n",
      "0  swa_a5748df181277341143f7da4175add4a             1\n",
      "1  swa_2df0d42f9b49ea2e4fb006b2e6604e6d             1\n",
      "2  swa_3718757514005767302b7220b08e409d             1\n",
      "3  swa_9fa3337a35cce723d60c06056d422330             1\n",
      "4  swa_5c39ac8ef70345e9e3c21a47f8769bc0             1\n"
     ]
    }
   ],
   "source": [
    "# Assuming your dev dataset has columns like 'sentence' and maybe 'label'\n",
    "# First, keep the original dev dataframe intact\n",
    "dev = pd.read_csv('dev/swa.csv')\n",
    "dev_original = dev.copy()\n",
    "\n",
    "# Tokenize the sentences\n",
    "dev_texts = dev['text'].tolist()  # This creates a list of texts\n",
    "dev_encodings = tokenizer(dev_texts, truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Create a dataset object for dev (without labels)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "dev_dataset = CustomDataset(dev_encodings)\n",
    "\n",
    "# Make predictions\n",
    "predictions = trainer.predict(dev_dataset)\n",
    "\n",
    "# Extract predicted labels\n",
    "predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Create the final dataframe with idx, sentence, prediction\n",
    "results = pd.DataFrame({\n",
    "    'id': dev_original['id'],  # Get 'id' from the original dataframe, not the list\n",
    "    'polarization': predicted_labels\n",
    "})\n",
    "\n",
    "# Display first few rows\n",
    "print(results.head())\n",
    "\n",
    "# Save to CSV if needed\n",
    "results.to_csv('pred_swa.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21394eea-733d-481f-bfa1-8e780ce916bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
